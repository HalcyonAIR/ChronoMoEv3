# ChronoMoEv3 Implementation Progress

**Status as of 2026-02-07**

---

## Summary

- **Phase 1:** ‚úÖ COMPLETE (Coherence computation core)
- **Architecture:** ‚úÖ COMPLETE (Three subsystems, 6 questions answered)
- **Sovereign Router:** ‚úÖ SPECIFIED (SC-004: Local/cloud split)
- **Phase 2:** üîÑ IN PROGRESS (Step 1 complete: RouterState + beta)
- **Next:** Step 2 implementation (Coherence on GPU)

---

## ‚úÖ Phase 1: Coherence Computation (COMPLETE)

**The foundation of v3. If phi_e doesn't track functional participation, nothing else matters.**

### Implemented

- ‚úÖ **MoETrace dataclass** ([`chronomoe_v3/coherence.py`](chronomoe_v3/coherence.py))
  - Canonical interface between MoE forward pass and coherence tracking
  - Works with both Mixtral-style (sequential loop) and Switch-style (batch)
  - Clean separation of router state, expert outputs, and mixture

- ‚úÖ **Coherence computation** (`phi_e = cosine(y_bar_e, y_bar_mix)`)
  - Per-expert coherence: Measures directional alignment with mixture
  - Layer-wide coherence: Weighted average (Psi_l)
  - Validated on perfect alignment (phi=1), opposite (phi=-1), orthogonal (phi=0)

- ‚úÖ **CoherenceState tracking** ([`chronomoe_v3/coherence.py`](chronomoe_v3/coherence.py))
  - Per-expert state with three-timescale EMAs
  - `phi_fast`, `phi_mid`, `phi_slow` persistence filters
  - Role vector tracking (what expert typically outputs)
  - Degradation detection via `phi_delta` (fast - slow)

- ‚úÖ **Three-clock system** ([`chronomoe_v3/clocks.py`](chronomoe_v3/clocks.py))
  - ClockConfig: alpha ‚Üî half_life conversion
  - ThreeClockEMA: Fast (~10 steps), Mid (~100 steps), Slow (~1000 steps)
  - Validated convergence and differential response rates

- ‚úÖ **Configuration** ([`chronomoe_v3/config.py`](chronomoe_v3/config.py))
  - ChronoConfig dataclass with all hyperparameters
  - Clock decay constants, slow bias, free energy weights
  - Edit selection policies, expert bounds

- ‚úÖ **Tests** ([`tests/`](tests/))
  - `test_coherence.py`: 11 tests covering trace, state, batch updates
  - `test_clocks.py`: 12 tests covering clock config, EMA, conversions
  - All passing ‚úÖ

- ‚úÖ **Demo** ([`examples/coherence_demo.py`](examples/coherence_demo.py))
  - Simulates 300 steps of MoE forward passes
  - Demonstrates expert degradation detection
  - Shows pruning and lifecycle transitions

### Key Insights

**Coherence is cheap:** <1-2% overhead. No extra matmuls, just means and cosines.

**Fast clock detects problems early:**
- Step 100: Expert 3 degrades, fast drops to 0.08 while slow still 0.095
- Step 110: `phi_delta = -0.013` (degrading flag triggered)
- Step 200: Expert pruned (no longer updated)

**Slow clock resists noise:**
- Healthy experts maintain slow coherence >0.25 after 300 steps
- Degraded experts show persistent negative delta

**Online mean computation works:**
- Mixtral-style: Reduces storage from 32MB/layer to 128KB/layer
- Switch-style: Extract from existing tensors (near-zero cost)

---

## ‚úÖ Architecture Phase (COMPLETE)

**Comprehensive architecture design and decision documentation completed.**

### Dataflow Analysis

- ‚úÖ **Mixtral MoE wiring** ([dataflow_mixtral.md](dataflow_mixtral.md))
  - Router ‚Üí expert ‚Üí mixture dataflow mapped
  - Hook points identified for coherence measurement
  - Sequential loop + index_add pattern documented

- ‚úÖ **Switch Transformer wiring** ([dataflow_switch_transformer.md](dataflow_switch_transformer.md))
  - Capacity-based dispatch analyzed
  - Einsum patterns documented
  - Batch-style expert computation advantages identified

- ‚úÖ **Comparison & recommendations** ([dataflow_comparison.md](dataflow_comparison.md))
  - Side-by-side analysis
  - ChronoMoEv3 design recommendations

- ‚úÖ **Reference patches** ([coherence_hooks.md](coherence_hooks.md))
  - MoETrace interface specification
  - Minimal hooks for both Mixtral and Switch patterns
  - Online mean computation (250√ó storage reduction)

### Architecture Decisions

- ‚úÖ **7 Critical questions answered** ([ARCHITECTURE_DECISIONS.md](ARCHITECTURE_DECISIONS.md))
  1. Slow bias location: Pre-softmax additive per expert ‚úì
  2. Checkpoint state: ~3MB for 64 experts (deterministic recovery) ‚úì
  3. Clean/biased disagreement: Hybrid escalation at 0.2/0.5/0.7 ‚úì
  4. Split + beta interaction: Relaxation trial protocol ‚úì
  5. z_clean computation: Explicit return in v3, hook for external ‚úì
  6. Falsification criterion: Low phi + high impact would invalidate ‚úì

- ‚úÖ **Architecture refinements** ([ARCHITECTURE_REFINEMENTS.md](ARCHITECTURE_REFINEMENTS.md))
  - Scale-free beta: k * logit_std (empirically validated) ‚úì
  - JS divergence vs top-1 flips (use both) ‚úì
  - Calibrated crisis thresholds (regime-adapted) ‚úì
  - Temperature interaction clarified ‚úì
  - Checkpoint ownership contracted ‚úì
  - Determinism guarantee softened (hysteresis added) ‚úì

### State Architecture

- ‚úÖ **State separation** ([STATE_SEPARATION.md](STATE_SEPARATION.md))
  - Jeff's yellow sticky note: role_vector ‚Üí lifecycle, not coherence
  - Clean boundary established

- ‚úÖ **Three subsystems** ([STATE_ARCHITECTURE_V2.md](STATE_ARCHITECTURE_V2.md))
  - **CoherenceState:** "Am I aligned?" (40 bytes, pure measurement)
  - **RoleState:** "What do I do?" (48KB, decision support)
  - **RouterState:** "What biases exist?" (5KB, routing infrastructure)
  - Lifecycle: Reader only, no accumulated state
  - One sentence per field discipline
  - No dumping grounds

### Empirical Validation

- ‚úÖ **Beta saturation analysis** ([experiments/beta_saturation_analysis.py](experiments/beta_saturation_analysis.py))
  - Safe range: |beta| ‚â§ 1.0 validated
  - At beta=1.0: 12% flip rate (moderate influence)
  - beta / logit_std = 0.35 (gentle prior)

- ‚úÖ **Scale-free beta validation** ([experiments/scale_free_beta_validation.py](experiments/scale_free_beta_validation.py))
  - Flip rate consistency: std=0.0026 ‚úì
  - JS divergence consistency: std=0.0029 ‚úì
  - Portability across regimes proven

---

## üèõÔ∏è Sovereign Router Architecture (SC-004)

**Foundational principle: Identity (local) vs Capability (cloud) split.**

### Specification

- ‚úÖ **Sovereignty Axiom** ([SOVEREIGN_ROUTER_ARCHITECTURE.md](SOVEREIGN_ROUTER_ARCHITECTURE.md))
  - "Everything that participates in commitment must be sovereign."
  - "Everything that provides capability can be shared."
  - All three temporal clocks (œÑ_f, œÑ_m, œÑ_s) must be local
  - Experts can be cloud-based and stateless

- ‚úÖ **Local Sovereign Core**
  - Router + 3 temporal keepers (one per clock)
  - Keepers = untrusted processes with limited kernel API access
  - Graceful degradation: "I'm still here but can't reach experts"

- ‚úÖ **Three Irreversible Identity Events**
  - Scar Formation (œÑ_s): "Never again" - reactive constraint
  - Reflex Crystallisation (œÑ_f): "Always this" - proactive habit (5 gates)
  - Epoch Boundary (œÑ_m): "Before & after" - developmental shift

- ‚úÖ **Kernel API**
  - Write budget enforcement (prevents runaway self-modification)
  - Two-step commit (proposals require persisting evidence)
  - Monotonicity (only identity-grade events are irreversible)
  - Tamper-evident audit log (locus drift must be debuggable)

- ‚úÖ **Geometric Foundations**
  - Fisher metric for all geometric computations
  - Geodesic dimensionality (not volume) measures freedom
  - Five-gate crystallisation protocol with anti-gaming safeguards
  - Expert load signatures (content-blind outcome proxy)

### Implementation Phases

**Keeper specifications:** Phase 6-7 (after lifecycle working)
**Crystallisation gates:** Phase 5+ (require lifecycle decisions)
**Kernel API:** Phase 7 (system integration)

---

## üìã Phase 2: Slow Bias (beta) (IN PROGRESS)

**The locus mechanism: persistent routing geometry without RAG.**

### Implementation Plan

**Complete 5-step vertical slice specified** ([PHASE2_IMPLEMENTATION_PLAN.md](PHASE2_IMPLEMENTATION_PLAN.md))

**Step 1:** ‚úÖ RouterState + beta application (COMPLETE)
- ‚úÖ RouterState with beta_coeff, logit_std_ema
- ‚úÖ Compute z_clean, z_biased
- ‚úÖ Route with z_biased
- ‚úÖ Disagreement metrics (JS divergence, flip rate)
- ‚úÖ Bridge detector with relevance modulation
- ‚úÖ Tests: 9 tests in test_router.py
- ‚úÖ Demo: step1_demo.py showing dual distribution

**Step 2:** üîÑ Coherence on GPU with buffered state (NEXT)
- CoherenceBuffer: GPU-resident tensors
- Update every step (no CPU sync bottleneck)
- Snapshot to CPU only on eval intervals

**Step 3:** Beta update function
- Simple rule: phi_slow < tau ‚Üí reduce beta, > tau ‚Üí increase
- Scale-free: normalize by logit_std_ema
- Clamp to [-k_max, k_max]

**Step 4:** Bridge detector veto
- Compute relevance scalar from overlap-only mass
- Modulate beta strength: beta_eff = r * beta_eff
- Prevent "Krypto from nowhere"

**Step 5:** Lifecycle coordinator (decisions only, dry-run)
- Detect prune candidates
- Log decisions, don't execute yet
- Starvation guardrail (Neff + saturation)

### Pre-Implementation Questions Answered

**All critical questions resolved** ([PHASE2_REFINEMENTS.md](PHASE2_REFINEMENTS.md))

1. ‚úÖ Beta sign: PROMOTION prior (high coherence ‚Üí beta increases)
2. ‚úÖ JS divergence: Per-token with 10% sampling
3. ‚úÖ File organization: coherence.py (API) + coherence/buffer.py (GPU)
4. ‚úÖ Relevance metric: Overlap-only mass (not just JS)
5. ‚úÖ Starvation signal: Neff + saturation proxy
6. ‚úÖ Stability criterion: 4 explicit assertions

**Timeline:** 5 days (1 step per day)

**Testing harness:** experiments/phase2_vertical_slice.py

### Why This Matters

The slow clock doesn't just measure ‚Äî it acts. Experts that persist through the slow window earn a routing advantage (`beta > 0`). Experts that fail to persist lose influence (`beta ‚Üí negative`). This is the trimming mechanism, in math.

---

## üìã Phase 3: Bimodality Detector (NOT STARTED)

**Detect "this expert is serving two basins."**

### To Implement

- [ ] Two-centroid tracking per expert
- [ ] Assignment and update logic (EMA on centroids)
- [ ] Separation √ó balance metric
- [ ] Integration with coherence state

### Why This Matters

High coherence doesn't mean healthy. An expert serving two incompatible basins can have decent average coherence but should split, not prune.

---

## üìã Phase 4: Free Energy Objective (NOT STARTED)

**Single objective replacing the rule bag.**

### To Implement

- [ ] `F_l = (1 - Psi_l) + lambda*N_l + rho*R_l + kappa*I_l`
- [ ] Misfit term (1 - Psi_l)
- [ ] Complexity tax (N_l)
- [ ] Redundancy detection (R_l)
- [ ] Instability penalty (I_l from bimodality)

---

## üìã Phase 5: Edit Proposal and Selection (NOT STARTED)

**Lifecycle as slow-clock physics.**

### To Implement

- [ ] Spawn: Add expert when layer starving
- [ ] Prune: Remove expert when irreversibly decoherent
- [ ] Split: Divide bimodal expert
- [ ] Merge: Combine redundant experts
- [ ] Candidate evaluation under F_l
- [ ] "Do nothing" threshold

---

## üìã Phase 6: Expert Registry (NOT STARTED)

**Fixed-width router with masking.**

### To Implement

- [ ] ExpertRegistry managing active/cooling/archived states
- [ ] Fixed-width router (max_experts per layer)
- [ ] Active mask for spawn/prune
- [ ] Optimizer state management on structural changes

---

## üìã Phase 7: ChronoSystem Integration (NOT STARTED)

**Wrap v2 Controller into unified system.**

### To Implement

- [ ] ChronoSystem class
- [ ] Single `step()` call for all three clocks
- [ ] Integration with ChronoMoEv2 Controller (mid clock)
- [ ] Decision logging (JSONL)

---

## üìã Phase 8: Benchmarks (NOT STARTED)

**Validate that this works.**

### To Implement

- [ ] Toy model: 8 experts, 2 layers, Shakespeare
- [ ] With vs without lifecycle
- [ ] F_l vs ad-hoc triggers
- [ ] Targeting correlation (like nanoMoE/Halcyon validation)

---

## Documentation Completed

- ‚úÖ [dataflow_mixtral.md](dataflow_mixtral.md) ‚Äî Mixtral wiring facts
- ‚úÖ [dataflow_switch_transformer.md](dataflow_switch_transformer.md) ‚Äî Switch wiring facts
- ‚úÖ [dataflow_comparison.md](dataflow_comparison.md) ‚Äî Comparison and recommendations
- ‚úÖ [coherence_hooks.md](coherence_hooks.md) ‚Äî Reference patches and MoETrace interface
- ‚úÖ [projectdesign.md](projectdesign.md) ‚Äî Full architectural specification
- ‚úÖ [firststeps.md](firststeps.md) ‚Äî Getting started guide

---

## Critical Path

**Phase 1 ‚Üí Phase 2 ‚Üí Phase 3 ‚Üí Phase 4 ‚Üí Phase 5**

Phase 1 (coherence) is the foundation. Everything else depends on `phi_e` tracking functional participation.

Phase 2 (slow bias) makes the locus real: persistent routing geometry that survives across prompts.

Phase 3 (bimodality) prevents misidentifying "serving two basins" as "low coherence."

Phase 4 (free energy) unifies spawn/prune/split/merge under one objective.

Phase 5 (lifecycle) implements the objective as slow-clock physics.

---

## Next Session

**Implement Phase 2: Slow Bias**

1. Create `SlowBias` class
2. Add `beta` parameter to router
3. Implement update rule
4. Validate persistence across "prompts" (batch boundaries)
5. Test that high-phi experts gain routing advantage

Then move to Phase 3 (bimodality detector).

---

**Status:** Phase 1 complete. v3 has a working heart. üéØ
