# ChronoMoEv3 Implementation Progress

**Status as of 2026-02-06**

---

## Summary

- **Phase 1:** âœ… COMPLETE (Coherence computation core)
- **Architecture:** âœ… COMPLETE (Three subsystems, 6 questions answered)
- **Phase 2 Plan:** âœ… READY (5-step vertical slice specified)
- **Next:** Begin Step 1 implementation (RouterState + beta application)

---

## âœ… Phase 1: Coherence Computation (COMPLETE)

**The foundation of v3. If phi_e doesn't track functional participation, nothing else matters.**

### Implemented

- âœ… **MoETrace dataclass** ([`chronomoe_v3/coherence.py`](chronomoe_v3/coherence.py))
  - Canonical interface between MoE forward pass and coherence tracking
  - Works with both Mixtral-style (sequential loop) and Switch-style (batch)
  - Clean separation of router state, expert outputs, and mixture

- âœ… **Coherence computation** (`phi_e = cosine(y_bar_e, y_bar_mix)`)
  - Per-expert coherence: Measures directional alignment with mixture
  - Layer-wide coherence: Weighted average (Psi_l)
  - Validated on perfect alignment (phi=1), opposite (phi=-1), orthogonal (phi=0)

- âœ… **CoherenceState tracking** ([`chronomoe_v3/coherence.py`](chronomoe_v3/coherence.py))
  - Per-expert state with three-timescale EMAs
  - `phi_fast`, `phi_mid`, `phi_slow` persistence filters
  - Role vector tracking (what expert typically outputs)
  - Degradation detection via `phi_delta` (fast - slow)

- âœ… **Three-clock system** ([`chronomoe_v3/clocks.py`](chronomoe_v3/clocks.py))
  - ClockConfig: alpha â†” half_life conversion
  - ThreeClockEMA: Fast (~10 steps), Mid (~100 steps), Slow (~1000 steps)
  - Validated convergence and differential response rates

- âœ… **Configuration** ([`chronomoe_v3/config.py`](chronomoe_v3/config.py))
  - ChronoConfig dataclass with all hyperparameters
  - Clock decay constants, slow bias, free energy weights
  - Edit selection policies, expert bounds

- âœ… **Tests** ([`tests/`](tests/))
  - `test_coherence.py`: 11 tests covering trace, state, batch updates
  - `test_clocks.py`: 12 tests covering clock config, EMA, conversions
  - All passing âœ…

- âœ… **Demo** ([`examples/coherence_demo.py`](examples/coherence_demo.py))
  - Simulates 300 steps of MoE forward passes
  - Demonstrates expert degradation detection
  - Shows pruning and lifecycle transitions

### Key Insights

**Coherence is cheap:** <1-2% overhead. No extra matmuls, just means and cosines.

**Fast clock detects problems early:**
- Step 100: Expert 3 degrades, fast drops to 0.08 while slow still 0.095
- Step 110: `phi_delta = -0.013` (degrading flag triggered)
- Step 200: Expert pruned (no longer updated)

**Slow clock resists noise:**
- Healthy experts maintain slow coherence >0.25 after 300 steps
- Degraded experts show persistent negative delta

**Online mean computation works:**
- Mixtral-style: Reduces storage from 32MB/layer to 128KB/layer
- Switch-style: Extract from existing tensors (near-zero cost)

---

## âœ… Architecture Phase (COMPLETE)

**Comprehensive architecture design and decision documentation completed.**

### Dataflow Analysis

- âœ… **Mixtral MoE wiring** ([dataflow_mixtral.md](dataflow_mixtral.md))
  - Router â†’ expert â†’ mixture dataflow mapped
  - Hook points identified for coherence measurement
  - Sequential loop + index_add pattern documented

- âœ… **Switch Transformer wiring** ([dataflow_switch_transformer.md](dataflow_switch_transformer.md))
  - Capacity-based dispatch analyzed
  - Einsum patterns documented
  - Batch-style expert computation advantages identified

- âœ… **Comparison & recommendations** ([dataflow_comparison.md](dataflow_comparison.md))
  - Side-by-side analysis
  - ChronoMoEv3 design recommendations

- âœ… **Reference patches** ([coherence_hooks.md](coherence_hooks.md))
  - MoETrace interface specification
  - Minimal hooks for both Mixtral and Switch patterns
  - Online mean computation (250Ã— storage reduction)

### Architecture Decisions

- âœ… **7 Critical questions answered** ([ARCHITECTURE_DECISIONS.md](ARCHITECTURE_DECISIONS.md))
  1. Slow bias location: Pre-softmax additive per expert âœ“
  2. Checkpoint state: ~3MB for 64 experts (deterministic recovery) âœ“
  3. Clean/biased disagreement: Hybrid escalation at 0.2/0.5/0.7 âœ“
  4. Split + beta interaction: Relaxation trial protocol âœ“
  5. z_clean computation: Explicit return in v3, hook for external âœ“
  6. Falsification criterion: Low phi + high impact would invalidate âœ“

- âœ… **Architecture refinements** ([ARCHITECTURE_REFINEMENTS.md](ARCHITECTURE_REFINEMENTS.md))
  - Scale-free beta: k * logit_std (empirically validated) âœ“
  - JS divergence vs top-1 flips (use both) âœ“
  - Calibrated crisis thresholds (regime-adapted) âœ“
  - Temperature interaction clarified âœ“
  - Checkpoint ownership contracted âœ“
  - Determinism guarantee softened (hysteresis added) âœ“

### State Architecture

- âœ… **State separation** ([STATE_SEPARATION.md](STATE_SEPARATION.md))
  - Jeff's yellow sticky note: role_vector â†’ lifecycle, not coherence
  - Clean boundary established

- âœ… **Three subsystems** ([STATE_ARCHITECTURE_V2.md](STATE_ARCHITECTURE_V2.md))
  - **CoherenceState:** "Am I aligned?" (40 bytes, pure measurement)
  - **RoleState:** "What do I do?" (48KB, decision support)
  - **RouterState:** "What biases exist?" (5KB, routing infrastructure)
  - Lifecycle: Reader only, no accumulated state
  - One sentence per field discipline
  - No dumping grounds

### Empirical Validation

- âœ… **Beta saturation analysis** ([experiments/beta_saturation_analysis.py](experiments/beta_saturation_analysis.py))
  - Safe range: |beta| â‰¤ 1.0 validated
  - At beta=1.0: 12% flip rate (moderate influence)
  - beta / logit_std = 0.35 (gentle prior)

- âœ… **Scale-free beta validation** ([experiments/scale_free_beta_validation.py](experiments/scale_free_beta_validation.py))
  - Flip rate consistency: std=0.0026 âœ“
  - JS divergence consistency: std=0.0029 âœ“
  - Portability across regimes proven

---

## ðŸ“‹ Phase 2: Slow Bias (beta) (READY TO START)

**The locus mechanism: persistent routing geometry without RAG.**

### Implementation Plan

**Complete 5-step vertical slice specified** ([PHASE2_IMPLEMENTATION_PLAN.md](PHASE2_IMPLEMENTATION_PLAN.md))

**Step 1:** RouterState + beta application (one layer)
- Add RouterState with beta_coeff, logit_std_ema
- Compute z_clean, z_biased
- Route with z_biased
- Log disagreement metrics (JS divergence, flip rate)

**Step 2:** Coherence on GPU with buffered state
- CoherenceBuffer: GPU-resident tensors
- Update every step (no CPU sync bottleneck)
- Snapshot to CPU only on eval intervals

**Step 3:** Beta update function
- Simple rule: phi_slow < tau â†’ reduce beta, > tau â†’ increase
- Scale-free: normalize by logit_std_ema
- Clamp to [-k_max, k_max]

**Step 4:** Bridge detector veto
- Compute relevance scalar from overlap-only mass
- Modulate beta strength: beta_eff = r * beta_eff
- Prevent "Krypto from nowhere"

**Step 5:** Lifecycle coordinator (decisions only, dry-run)
- Detect prune candidates
- Log decisions, don't execute yet
- Starvation guardrail (Neff + saturation)

### Pre-Implementation Questions Answered

**All critical questions resolved** ([PHASE2_REFINEMENTS.md](PHASE2_REFINEMENTS.md))

1. âœ… Beta sign: PROMOTION prior (high coherence â†’ beta increases)
2. âœ… JS divergence: Per-token with 10% sampling
3. âœ… File organization: coherence.py (API) + coherence/buffer.py (GPU)
4. âœ… Relevance metric: Overlap-only mass (not just JS)
5. âœ… Starvation signal: Neff + saturation proxy
6. âœ… Stability criterion: 4 explicit assertions

**Timeline:** 5 days (1 step per day)

**Testing harness:** experiments/phase2_vertical_slice.py

### Why This Matters

The slow clock doesn't just measure â€” it acts. Experts that persist through the slow window earn a routing advantage (`beta > 0`). Experts that fail to persist lose influence (`beta â†’ negative`). This is the trimming mechanism, in math.

---

## ðŸ“‹ Phase 3: Bimodality Detector (NOT STARTED)

**Detect "this expert is serving two basins."**

### To Implement

- [ ] Two-centroid tracking per expert
- [ ] Assignment and update logic (EMA on centroids)
- [ ] Separation Ã— balance metric
- [ ] Integration with coherence state

### Why This Matters

High coherence doesn't mean healthy. An expert serving two incompatible basins can have decent average coherence but should split, not prune.

---

## ðŸ“‹ Phase 4: Free Energy Objective (NOT STARTED)

**Single objective replacing the rule bag.**

### To Implement

- [ ] `F_l = (1 - Psi_l) + lambda*N_l + rho*R_l + kappa*I_l`
- [ ] Misfit term (1 - Psi_l)
- [ ] Complexity tax (N_l)
- [ ] Redundancy detection (R_l)
- [ ] Instability penalty (I_l from bimodality)

---

## ðŸ“‹ Phase 5: Edit Proposal and Selection (NOT STARTED)

**Lifecycle as slow-clock physics.**

### To Implement

- [ ] Spawn: Add expert when layer starving
- [ ] Prune: Remove expert when irreversibly decoherent
- [ ] Split: Divide bimodal expert
- [ ] Merge: Combine redundant experts
- [ ] Candidate evaluation under F_l
- [ ] "Do nothing" threshold

---

## ðŸ“‹ Phase 6: Expert Registry (NOT STARTED)

**Fixed-width router with masking.**

### To Implement

- [ ] ExpertRegistry managing active/cooling/archived states
- [ ] Fixed-width router (max_experts per layer)
- [ ] Active mask for spawn/prune
- [ ] Optimizer state management on structural changes

---

## ðŸ“‹ Phase 7: ChronoSystem Integration (NOT STARTED)

**Wrap v2 Controller into unified system.**

### To Implement

- [ ] ChronoSystem class
- [ ] Single `step()` call for all three clocks
- [ ] Integration with ChronoMoEv2 Controller (mid clock)
- [ ] Decision logging (JSONL)

---

## ðŸ“‹ Phase 8: Benchmarks (NOT STARTED)

**Validate that this works.**

### To Implement

- [ ] Toy model: 8 experts, 2 layers, Shakespeare
- [ ] With vs without lifecycle
- [ ] F_l vs ad-hoc triggers
- [ ] Targeting correlation (like nanoMoE/Halcyon validation)

---

## Documentation Completed

- âœ… [dataflow_mixtral.md](dataflow_mixtral.md) â€” Mixtral wiring facts
- âœ… [dataflow_switch_transformer.md](dataflow_switch_transformer.md) â€” Switch wiring facts
- âœ… [dataflow_comparison.md](dataflow_comparison.md) â€” Comparison and recommendations
- âœ… [coherence_hooks.md](coherence_hooks.md) â€” Reference patches and MoETrace interface
- âœ… [projectdesign.md](projectdesign.md) â€” Full architectural specification
- âœ… [firststeps.md](firststeps.md) â€” Getting started guide

---

## Critical Path

**Phase 1 â†’ Phase 2 â†’ Phase 3 â†’ Phase 4 â†’ Phase 5**

Phase 1 (coherence) is the foundation. Everything else depends on `phi_e` tracking functional participation.

Phase 2 (slow bias) makes the locus real: persistent routing geometry that survives across prompts.

Phase 3 (bimodality) prevents misidentifying "serving two basins" as "low coherence."

Phase 4 (free energy) unifies spawn/prune/split/merge under one objective.

Phase 5 (lifecycle) implements the objective as slow-clock physics.

---

## Next Session

**Implement Phase 2: Slow Bias**

1. Create `SlowBias` class
2. Add `beta` parameter to router
3. Implement update rule
4. Validate persistence across "prompts" (batch boundaries)
5. Test that high-phi experts gain routing advantage

Then move to Phase 3 (bimodality detector).

---

**Status:** Phase 1 complete. v3 has a working heart. ðŸŽ¯
